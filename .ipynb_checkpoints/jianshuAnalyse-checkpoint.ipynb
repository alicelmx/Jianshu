{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据清洗和绘图观察数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 're' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f9050f4cb158>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Month'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'YMD'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetMonth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Day'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'YMD'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetDay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Hour'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Time'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetHour\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   2549\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2550\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2551\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2553\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/src/inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-f9050f4cb158>\u001b[0m in \u001b[0;36mgetHour\u001b[0;34m(timestamp)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgetHour\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimestamp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mrawhour\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimestamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'T'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'+'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mhour\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'^(0+)'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrawhour\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhour\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mhour\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 're' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 查看数据的基本格式\n",
    "df = pd.read_csv('./JianShuJiaoYou-All-Data.csv',encoding='utf-8')\n",
    "\n",
    "'''\n",
    "对数据进行预处理\n",
    "'''\n",
    "# eval函数：就是实现list、dict、tuple与str之间的转化\n",
    "# 新增'Images_Num'列，存储每篇文章中的配图数目\n",
    "def getImagesNum(imageList):\n",
    "    return len(eval(imageList))\n",
    "\n",
    "# 统计下每篇文章的字数\n",
    "def getArticleLength(articleContent):\n",
    "    return len(articleContent)\n",
    "\n",
    "# 对Time属性进行拆分\n",
    "def getYMD(timestamp):\n",
    "    ymd = timestamp.split('T')[0]\n",
    "    return ymd\n",
    "\n",
    "def getYear(ymd):\n",
    "    year = ymd.split('-')[0]\n",
    "    return year\n",
    "\n",
    "def getMonth(ymd):\n",
    "    month = ymd.split('-')[1]\n",
    "    return month\n",
    "\n",
    "def getDay(ymd):\n",
    "    day = ymd.split('-')[-1]\n",
    "    return day\n",
    "\n",
    "def getHour(timestamp):\n",
    "    rawhour = timestamp.split('T')[1].split('+')[0][:2]     \n",
    "    hour = int(re.sub(r'^(0+)','',str(rawhour)))\n",
    "    if hour == '':\n",
    "        hour =0\n",
    "    return hour\n",
    "\n",
    "df['Images_Num'] = df['Image_Url'].apply(getImagesNum)\n",
    "df['Artical_Length'] = df['Artical_Content'].apply(getArticleLength)\n",
    "df['YMD'] = df['Time'].apply(getYMD)\n",
    "df['Year'] = df['YMD'].apply(getYear)\n",
    "df['Month'] = df['YMD'].apply(getMonth)\n",
    "df['Day'] = df['YMD'].apply(getDay)\n",
    "df['Hour'] = df['Time'].apply(getHour)\n",
    "\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "# 解决中文显示问题\n",
    "plt.rcParams['font.sans-serif'] = ['STHeiti']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 内嵌画图，可以省去plt.show()\n",
    "%matplotlib inline\n",
    "plt.hist(df['Images_Num'],alpha=0.5)\n",
    "plt.xlabel('配图数量')\n",
    "plt.ylabel('文章数量')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 显示文章长度和文章数量的关系\n",
    "print(df['Artical_Length'].value_counts())\n",
    "plt.hist(df['Artical_Length'],alpha=0.5)\n",
    "plt.xlabel('文章字数')\n",
    "plt.ylabel('文章数量')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 绘图显示发文章时间规律\n",
    "print(df['Hour'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看文章字数和配图数的关系\n",
    "plt.scatter(df['Artical_Length'],df['Images_Num'],marker='o',color='blue')\n",
    "plt.xlabel('文章字数')\n",
    "plt.ylabel('文章配图数量')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘图查看评论数和点赞数的关系\n",
    "plt.scatter(df['Comment'],df['Like'],marker='o',color='green')\n",
    "plt.xlabel('文章评论数')\n",
    "plt.ylabel('文章点赞数')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看阅读量、评论数与点赞数这三者的关系\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# 创建一个3维绘图工程\n",
    "plt.figure(figsize=(12, 7))\n",
    "ax = plt.subplot(111,projection='3d')\n",
    "ax.scatter(df['Comment'],df['Like'],df['Comment'],s=15)\n",
    "\n",
    "ax.set_title('阅读量、评论、喜欢三者关系')\n",
    "ax.set_zlabel('评论')\n",
    "ax.set_ylabel('喜欢')\n",
    "ax.set_xlabel('评论')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 提取文章中200个关键词绘制词云"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# 忽略警告\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "content = df['Artical_Content'].tolist()\n",
    "# print(len(content),'\\n',content[-1])\n",
    "content[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 载入停用词\n",
    "# quoting : int or csv.QUOTE_* instance, default 0\n",
    "# 控制csv中的引号常量。可选 QUOTE_MINIMAL (0), QUOTE_ALL (1), QUOTE_NONNUMERIC (2) or QUOTE_NONE (3)\n",
    " \n",
    "stopwords = pd.read_csv('./chinesestopword.txt',sep='\\n',encoding='utf-8',names=['stopwords'],header=None,quoting=3)\n",
    "# 默认输出5个\n",
    "stopwords.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 结巴分词\n",
    "import re\n",
    "import jieba\n",
    "pattern = re.compile('\\d+')\n",
    "\n",
    "from collections import Counter\n",
    "# 存放词语和词频\n",
    "wordsCounter = Counter()\n",
    "\n",
    "for line in content:\n",
    "    segs = jieba.lcut(line)\n",
    "    for seg in segs:\n",
    "        if len(seg)>1 and seg != '\\r\\n' and re.search(pattern,seg)==None:\n",
    "            wordsCounter[seg] += 1\n",
    "# print(wordsCounter.most_common(100))\n",
    "# 将Counter的键提取出来做list\n",
    "segment = list(wordsCounter)\n",
    "\n",
    "# 将分好的词列表转化为词典\n",
    "words = pd.DataFrame({'segment':segment})\n",
    "\n",
    "# 剔除停用词\n",
    "words = words[~words['segment'].isin(stopwords['stopwords'])]\n",
    "# words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 绘制词云\n",
    "from pyecharts import WordCloud\n",
    "\n",
    "def counter2list(_counter):\n",
    "    wordslist,nums = [],[]\n",
    "    for item in _counter:\n",
    "        wordslist.append(item[0])\n",
    "        nums.append(item[1])\n",
    "    return wordslist,nums\n",
    "\n",
    "outputFile = './result/文章关键词词云图.html'\n",
    "\n",
    "# 这个关键词抽取方法不唯一\n",
    "wordslist,nums = counter2list(wordsCounter.most_common(1000))\n",
    "\n",
    "cloud = WordCloud('文章关键词词云', width=1200, height=600, title_pos='center')\n",
    "cloud.add(\n",
    "    ' ',wordslist,nums,\n",
    "    shape='circle',\n",
    "    background_color='white',\n",
    "    max_words=1000 \n",
    ")\n",
    "cloud.render(outputFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec模型计算词的相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba.analyse\n",
    "\n",
    "# 基于TF-IDF算法的关键词抽取\n",
    "contentStr = ''.join(content)\n",
    "# keyWordsList = jieba.analyse.extract_tags(contentStr,topK=200,allowPOS=('ns','n'))\n",
    "# textrank1 = ' '.join(keyWordsList)\n",
    "# textrank1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基于TextRank算法进行关键词抽取\n",
    "textrank2 = ' '.join(jieba.analyse.textrank(contentStr,topK=200,allowPOS=('ns','n')))\n",
    "textrank2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 利用word2vec找出关联词语，将语料整理成嵌套列表的形式\n",
    "corpus = []\n",
    "\n",
    "# 将停用词dataFrame转化为列表\n",
    "stopwordsList = stopwords['stopwords'].tolist()\n",
    "# print(stopwordsList[:10])\n",
    "for line in content:\n",
    "    segs = jieba.lcut(line)\n",
    "    segs = filter(lambda x:len(x)>1, segs)\n",
    "    segs = filter(lambda x:re.search(pattern,x)==None, segs)\n",
    "    segs = filter(lambda x:x not in stopwordsList, segs)\n",
    "    \n",
    "    corpus.append(list(segs))\n",
    "    \n",
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 训练模型\n",
    "import multiprocessing\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(corpus,min_count=20,sg=0,workers=multiprocessing.cpu_count())\n",
    "# 查询和简书相关性比较高的词语\n",
    "model.wv.most_similar(['简书'],topn=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE 可视化高维数据\n",
    "t-SNE是目前最为流行的一种高维数据降维的算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 调用百度云NLP进行词向量表示\n",
    "from aip import AipNlp\n",
    "\n",
    "APP_ID = '11617353'\n",
    "API_KEY = 'eV2R48IOWKcLgBrZwtf0ZF7N'\n",
    "SECRET_KEY = 'HHtuGb3BPGaXAguPld5r9gfrY4xCCdzh'\n",
    "client = AipNlp(APP_ID, API_KEY, SECRET_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取textrank4中Top200词语的词向量\n",
    "textrankList = textrank2.split(' ')\n",
    "textrankList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 整理格式 原始格式：('简书', 0.9286492277441794) 后者是特征权重\n",
    "import numpy as np\n",
    "words_list = []\n",
    "word_vectors = []\n",
    "\n",
    "for word in textrankList:\n",
    "    try:\n",
    "        data = client.wordEmbedding(word)\n",
    "#         print(data)\n",
    "        word_vector = data['vec']\n",
    "        words_list.append(data['word'])\n",
    "        word_vectors.append(word_vector)\n",
    "    except:\n",
    "        print('No words:{}'.format(word))\n",
    "    \n",
    "word_vectors = np.array(word_vectors)\n",
    "print(words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "\n",
    "def plotTsne2D(word_vectors,words_list):\n",
    "    tsne = TSNE(n_components=2,random_state=0,n_iter=10000,perplexity=20)\n",
    "    # 在控制台输出过程中，默认小数会以科学计数法的形式输出，若不需要加上下面这句\n",
    "    np.set_printoptions(suppress=True)\n",
    "    T = tsne.fit_transform(word_vectors)\n",
    "    labels = wordslist\n",
    "    \n",
    "    plt.figure(figsize=(14,10))\n",
    "    plt.scatter(T[:,0],T[:,1],c='blue',edgecolors='k')\n",
    "    \n",
    "    for label,x,y in zip(labels,T[:,0],T[:,1]):\n",
    "        plt.annotate(label,xy=(x+1,y+1),xytext=(0,0),textcoords='offset points')\n",
    "\n",
    "plotTsne2D(word_vectors,words_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 利用LDA算法进行主题词提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "# for article in corpus:\n",
    "dictionary = corpora.Dictionary(corpus)\n",
    "# 将 dictionary 转化为一个词袋\n",
    "common_corpus = [dictionary.doc2bow(text) for text in corpus]\n",
    "tfidf = models.TfidfModel(common_corpus)\n",
    "corpusTfidf = tfidf[common_corpus]\n",
    "\n",
    "lda = LdaModel(corpusTfidf, num_topics=10, id2word = dictionary, passes=20)\n",
    "\n",
    "results = lda.print_topics(num_topics=10, num_words=3)\n",
    "for res in results:\n",
    "    print(res[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将LDA模型保存起来方便日后对新文章进行主题预测\n",
    "lda.save('lda.model')\n",
    "test = \"生活是一座围城，但校园不是。校园是时光精心雕刻的礼物，里面到处有十几岁长发飘飘的女孩子，朝气执着的男孩子，有单纯追求的理想和生活，还有那些用心守候的友情爱情。秋天的梧桐树铺满教学楼的道路，在密密麻麻的习题中痴迷地偷看窗外的世界，静谧、自由；冬天的大雪来势汹汹，裹着厚棉袄走在冰封的湖边独自去上课，长大的世界只有自己可以取暖；春天，随着冰雪一起褪去的还有沉重的身心，但新的一年生活好像也没什么变化；夏天有吃不完的西瓜和冰棍，如果你可以和我一起逃课，我会告诉你后山夜晚的星星很美。提起校园，或许它并不精彩，重复到近乎单调，但我们都会无比怀念年轻的自己和那些有梦做的年纪。春天濛濛的细雨还没有离开大地，夏日离别的笙歌已在悄悄响起。又快到一年毕业季，「简书交友」专题携手「简书校园」、「简书会员」、「摄影」一起推出「恰同学少年」校园创作交友大赛。请将文章以以下形式投稿至「」专题、摄影作品可投稿至「」专题。本次活动设置了丰富的奖项和奖品，参赛内容可为小学到大学，培训学校等等你经历过的任何一个校园，校园必须为的大学，虚拟意义上的校园无法参赛，，所有简友仅需写出你曾经经历过的校园生活即可参赛。感谢对本次校园创作交友大赛的，除了提供奖金支持以外，还带来了八折的购买折扣。已是简书会员的简友参与本次比赛即可线上时长；所有参与本次比赛的简友均可获得购买简书会员的资格。详细会员权益请见：参加摄影奖的文章不参考文章质量，仅参考摄影作品，但建议对摄影作品加上一定的文字描述。片子必须原创，拒绝糊片，张数不限，可用snapseed、PS等软件修图，一旦发现盗图，永久取消参与简书活动的资格；一等奖一名：500元二等奖三名：200元三等奖五名：50元创作奖视文字部分内容而定，摄影仅做锦上添花的辅助说明。一等奖二名：1000元二等奖三名：500元三等奖五名：200元凡在本次比赛参与作品中找到同校同学，你们可以将自己的互相发布到彼此的文章，只要有五名以上的同校同学，你可以获得19元打赏。每人仅能获得一次且必须在自己的文章评论区集齐五名校友评论。收集齐后点击链接附上你的文章链接和校友评论楼层，通过验证即可获奖。登记地址：交友专题会对你提供的文章进行质量审核，单纯为了凑齐人数而没达到参赛标准的文章将被视为不符合规则。本次比赛人气最高的五篇文章和点赞人气前三的评论作者都可以获得人气奖，人气奖可与以上奖项重复获得。文章评分：0.4阅读量+0.3 评论数+0.3 点赞数，得分取前5名。人气奖奖品为：100元2018年3月27日-2018年4月20日，预计2018年5月上旬公布结果，获奖结果首发（微信号：jianshuio）。1.文章建议采用记叙性、回忆性散文、诗歌，校园生活和校园情，请勿全文叙述你的爱情（同理友情、师生情等等），不接受小说等虚构类作品；2.快速找到校友：点击列表找到自己的学校，或者点击联系负责人加入学校社群。结交校友可以100%获得。3.活动不局限于在校生参与，任何简友都可以写出你憧憬/经历过的校园生活；4.简书对所有参赛文章具有使用权，简书交友专题对活动具有最终解释权；「恰同学少年」校园创作交友大赛同步合作伙伴、引力说（微信号：GravityYLS）、清华帮（微信号：THU_bang）、小也电台（微信号：xiaoyeradio），获奖作品将同步发表在以上平台。加入简书第一步，添加简书交友官方微信群（已在前面九群的请不要重复添加）。\"\n",
    "\n",
    "test = jieba.lcut(test)\n",
    "\n",
    "# 文档转换成bow\n",
    "doc_bow = dictionary.doc2bow(test)\n",
    "# 得到新文档的主题分布\n",
    "doc_lda = lda[doc_bow]\n",
    "type = doc_lda[1][0]\n",
    "print(results[type][1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
